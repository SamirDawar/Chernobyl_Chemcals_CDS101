---
title: "CDS 101 – Final Project Report"
author: "Samir Dawar, Andrew Lee, Sumera"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output:
  html_document:
    css: "gmu-cds101.css"
    toc: true
    toc_depth: 3
    number_sections: true
  pdf_document:
    toc: true
---


# 1. Problem Definition
The Chernobyl nuclear disaster in April 1986 released large amounts of radioactive isotopes into the atmosphere, which were carried across Europe by wind patterns and weather systems. Understanding how far these contaminants traveled and how their concentrations changed with distance is essential for public health, environmental science, and historical analysis of nuclear fallout.
In this project, we investigate how air concentrations of three isotopes—Cs-137, Cs-134, and I-131—vary with distance from the Chernobyl Nuclear Power Plant. Cs-137 is a long-lived isotope (half-life ~30 years), Cs-134 has a shorter half-life (~2 years), and I-131 is very short-lived (~8 days). Because these isotopes behave differently over time, comparing them allows us to see which fallout patterns were short-term versus long-term.
Our primary goal is to quantify the relationship between distance and isotope concentrations using exploratory visualizations and three simple linear regression models. We expect Cs-137 to show the clearest distance-related decline, Cs-134 to show a weaker pattern, and I-131 to show little or no distance trend due to rapid decay.



# 2. Data Acquisition & Description
We use the “Chernobyl Chemical Radiation / CSV / Country Data” dataset from Kaggle, which compiles air-concentration measurements of radioactive isotopes from monitoring stations across Europe in 1986.

We downloaded the dataset and saved it as `Chernobyl_Chemical_Radiation.csv` in the `data/` folder. Each row represents a measurement at a specific location on a specific date.

Key variables:

- `PAYS`: two-letter country code  
- `Location`: monitoring station name or code  
- `Longitude`: longitude of the station  
- `Latitude`: latitude of the station  
- `Date`: measurement date  
- `I_131_(Bq/m3)`: I-131 concentration  
- `Cs_134_(Bq/m3)`: Cs-134 concentration  
- `Cs_137_(Bq/m3)`: Cs-137 concentration  

Units for isotopes are Bq/m³ (becquerels per cubic meter).

These measurements were collected shortly after the disaster and reflect atmospheric radiation levels across Europe. There are no privacy concerns, as the dataset contains only environmental monitoring station data, not personal data.

```{r setup-data, echo=FALSE, message=FALSE, warning=FALSE}
# Example: load a dataset (replace with your own)
# library(readr)
# data <- read_csv("data/your_data.csv")
```

# 3. Data Cleaning & Preprocessing

Describe the steps you took to make the data usable, for example:

- Handling missing values (drop, impute, etc.).  
- Fixing types (dates, numeric vs. categorical).  
- Removing outliers (if done).  
- Creating new features (feature engineering).  
- Any scaling or encoding.

```{r cleaning, eval=FALSE}
# Example pseudo-code for cleaning:
# data_clean <- data |>
#   dplyr::filter(!is.na(target_variable)) |>
#   dplyr::mutate(
#     new_feature = some_transformation(old_feature)
#   )
```

# 4. Exploratory Data Analysis (EDA)

This section maps directly to the **EDA** part of the rubric:

- Summary statistics  
- Visualizations  
- Interpretation connected to the research question  

```{r eda-summary, message=FALSE, warning=FALSE}
# library(dplyr)
# summary(data_clean)
```

```{r eda-plot-example, message=FALSE, warning=FALSE, fig.cap="Example histogram of a numeric variable (EDA rubric)."}
# library(ggplot2)
# ggplot(data_clean, aes(x = some_numeric_variable)) +
#   geom_histogram(binwidth = 5, color = "white") +
#   labs(x = "Some Variable", y = "Count", title = "Distribution of Some Variable")
```

# 5. Visualization Quality and Storytelling

Use this section to satisfy the **Visualization Quality** rubric criterion:

- Explain why your plot types are appropriate.  
- Comment on labels, legends, colors, and overall readability.  
- Mention any steps you took to make plots accessible and interpretable.

# 6. Modeling Approach

Explain how you framed the problem and which models you chose:

- Type of task (regression, classification, etc.).  
- Baseline model or heuristic, if used.  
- Main model(s) chosen and why they are appropriate.

# 7. Model Implementation & Evaluation

This corresponds to the **Model Implementation & Evaluation** rubric criterion.

Describe:

- Features used.  
- Data splitting strategy (train/test or cross-validation).  
- Metrics used (accuracy, F1, RMSE, etc.).  
- Tables/plots summarizing performance.  
- A short interpretation of each metric/plot.

```{r modeling, eval=FALSE}
# Example structure:
# set.seed(123)
# train_index <- sample(seq_len(nrow(data_clean)), size = 0.8 * nrow(data_clean))
# train <- data_clean[train_index, ]
# test  <- data_clean[-train_index, ]
#
# model <- glm(target ~ ., data = train, family = binomial)
# preds <- predict(model, newdata = test, type = "response")
# # compute metrics...
```

# 8. Conclusions & Recommendations

Summarize the key takeaways:

- Answer your original research question(s) directly.  
- Highlight the most important patterns or relationships you found.  
- Discuss limitations (data size, bias, missing variables, etc.).  
- Suggest possible extensions or future work.

# 9. Code Quality & Reproducibility

Briefly document how someone else can reproduce your results:

- Which R scripts or Rmd files to run.  
- Any required R packages.  

```{r session-info, echo=FALSE}
sessionInfo()
```

# 10. References

List any references you used, such as:

- Dataset documentation.  
- Research papers or articles.  
- Tutorials or blog posts.

# Appendix (Optional)

Include extra plots, diagnostic checks, or model comparisons if needed.
